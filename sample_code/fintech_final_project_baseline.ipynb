{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Fintech final project baseline \n",
        "-----\n",
        "本 notebook 作為 fintech 金融科技導論的期末專題競賽 baseline 程式說明。\n",
        "\n",
        "* [競賽連結](https://tbrain.trendmicro.com.tw/Competitions/Details/24)\n",
        "\n",
        "首先會就資料格式以及處理說明，接續簡介模型訓練,最終預測結果並輸出目標格式。"
      ],
      "metadata": {
        "id": "eQw_ekfgCUP0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reminder: XGBoost 版本會影響 performance，請同學多注意。"
      ],
      "metadata": {
        "id": "XW_SVakcZznj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xgboost==1.7.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "va46Qjc-n--k",
        "outputId": "a07e42db-f1a8-4474-cc1f-f09792122929"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting xgboost==1.7.1\n",
            "  Downloading xgboost-1.7.1-py3-none-manylinux2014_x86_64.whl (193.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 193.6 MB 52 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from xgboost==1.7.1) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from xgboost==1.7.1) (1.7.3)\n",
            "Installing collected packages: xgboost\n",
            "  Attempting uninstall: xgboost\n",
            "    Found existing installation: xgboost 0.90\n",
            "    Uninstalling xgboost-0.90:\n",
            "      Successfully uninstalled xgboost-0.90\n",
            "Successfully installed xgboost-1.7.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "xgboost"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import library\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import collections\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "7vk_2W7kFHVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 資料前處理\n",
        "這邊針對訓練資料和測試的資料作整理。\n",
        "baseline主要會使用到的csv檔案如下:\n",
        "  - public_train_custinfo_full_hashed.csv: 包含主要要判斷的alert key對應的幾項參數，顧客id, 風險等級, 職業, 行內總資產和年齡。\n",
        "  - train_x_alert_date: 作為訓練資料的alert key以及發生日期，共23906筆。\n",
        "  - public_x_alert_date: 作為公開測試集的alert key，格式同上共1845筆。\n",
        "  - train_y_answer: 訓練資料alert key對應最後是否SAR。\n",
        "  - 預測的案件名單及提交檔案範例: 用於生成預測結果\n",
        "\n",
        "除此之外，還會使用到顧客資訊當作訓練資料:\n",
        "  - public_train_x_ccba_full_hashed.csv\n",
        "  - public_train_x_cdtx0001_full_hashed.csv\n",
        "  - public_train_x_dp_full_hashed.csv\n",
        "  - public_train_x_remit1_full_hashed.csv\n",
        "\n",
        "前處理的方式包含:\n",
        "  - 從 alert key 檢索出顧客資訊\n",
        "  - 對非數值 feature 做 label encoding\n",
        "  - 從顧客資訊中挑選適合的 features 當作訓練資料，這裡挑選離 alert date 最近的一筆顧客資訊當作 features\n",
        "  - 統計 training data 缺失值數量"
      ],
      "metadata": {
        "id": "X7J4yqaPVwJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(data_dir):\n",
        "    # declare csv path\n",
        "    train_alert_date_csv = os.path.join(data_dir, 'train_x_alert_date.csv')\n",
        "    cus_info_csv = os.path.join(data_dir, 'public_train_x_custinfo_full_hashed.csv')\n",
        "    y_csv = os.path.join(data_dir, 'train_y_answer.csv')\n",
        "\n",
        "    ccba_csv = os.path.join(data_dir, 'public_train_x_ccba_full_hashed.csv')\n",
        "    cdtx_csv = os.path.join(data_dir, 'public_train_x_cdtx0001_full_hashed.csv')\n",
        "    dp_csv = os.path.join(data_dir, 'public_train_x_dp_full_hashed.csv')\n",
        "    remit_csv = os.path.join(data_dir, 'public_train_x_remit1_full_hashed.csv')\n",
        "\n",
        "    public_x_csv = os.path.join(data_dir, 'public_x_alert_date.csv')\n",
        "\n",
        "    cus_csv = [ccba_csv, cdtx_csv, dp_csv, remit_csv]\n",
        "    date_col = ['byymm', 'date', 'tx_date', 'trans_date']\n",
        "    data_use_col = [[1,3,4,5,6,7,8,9],[2,3,4],[1,4,5,6,7,8,9,10,11],[2,3]]\n",
        "    \n",
        "    print('Reading csv...')\n",
        "    # read csv\n",
        "    df_y = pd.read_csv(y_csv)\n",
        "    df_cus_info = pd.read_csv(cus_info_csv)\n",
        "    df_date = pd.read_csv(train_alert_date_csv)\n",
        "    cus_data = [pd.read_csv(_x) for _x in cus_csv]\n",
        "    df_public_x = pd.read_csv(public_x_csv)\n",
        "\n",
        "    # do label encoding\n",
        "    le = LabelEncoder()\n",
        "    cus_data[2].debit_credit = le.fit_transform(cus_data[2].debit_credit)\n",
        "\n",
        "    \n",
        "\n",
        "    \n",
        "    cnts = [0] * 4\n",
        "    labels = []\n",
        "    training_data = []\n",
        "\n",
        "    print('Start processing training data...')\n",
        "    start = time.time()\n",
        "    for i in range(df_y.shape[0]):\n",
        "        # from alert key to get customer information\n",
        "        cur_data = df_y.iloc[i]\n",
        "        alert_key, label = cur_data['alert_key'], cur_data['sar_flag']\n",
        "\n",
        "        cus_info = df_cus_info[df_cus_info['alert_key']==alert_key].iloc[0]\n",
        "        cus_id = cus_info['cust_id']\n",
        "        cus_features = cus_info.values[2:]\n",
        "\n",
        "        date = df_date[df_date['alert_key']==alert_key].iloc[0]['date']\n",
        "\n",
        "\n",
        "        cnt = 0\n",
        "        for item, df in enumerate(cus_data):\n",
        "            cus_additional_info = df[df['cust_id']==cus_id]\n",
        "            cus_additional_info = cus_additional_info[cus_additional_info[date_col[item]]<=date]\n",
        "\n",
        "            if cus_additional_info.empty:\n",
        "                cnts[item] += 1\n",
        "                len_item = len(data_use_col[item])\n",
        "                if item == 2:\n",
        "                    len_item -= 1\n",
        "                cus_features = np.concatenate((cus_features, [np.nan] * len_item), axis=0)\n",
        "            else:\n",
        "                cur_cus_feature = cus_additional_info.loc[cus_additional_info[date_col[item]].idxmax()]\n",
        "                \n",
        "                cur_cus_feature = cur_cus_feature.values[data_use_col[item]]\n",
        "                # 處理 實際金額 = 匯率*金額\n",
        "                if item == 2:\n",
        "                    cur_cus_feature = np.concatenate((cur_cus_feature[:2], [cur_cus_feature[2]*cur_cus_feature[3]], cur_cus_feature[4:]), axis=0)\n",
        "                cus_features = np.concatenate((cus_features, cur_cus_feature), axis=0)\n",
        "        labels.append(label)\n",
        "        training_data.append(cus_features)\n",
        "        print('\\r processing data {}/{}'.format(i+1, df_y.shape[0]), end = '')\n",
        "    print('Processing time: {:.3f} secs'.format(time.time()-start))\n",
        "    print('Missing value of 4 csvs:', cnts)\n",
        "\n",
        "\n",
        "    print('Start processing testing data')\n",
        "    testing_data, testing_alert_key = [], []\n",
        "    for i in range(df_public_x.shape[0]):\n",
        "        # from alert key to get customer information\n",
        "        cur_data = df_public_x.iloc[i]\n",
        "        alert_key, date = cur_data['alert_key'], cur_data['date']\n",
        "\n",
        "        cus_info = df_cus_info[df_cus_info['alert_key']==alert_key].iloc[0]\n",
        "        cus_id = cus_info['cust_id']\n",
        "        cus_features = cus_info.values[2:]\n",
        "\n",
        "        for item, df in enumerate(cus_data):\n",
        "            cus_additional_info = df[df['cust_id']==cus_id]\n",
        "            cus_additional_info = cus_additional_info[cus_additional_info[date_col[item]]<=date]\n",
        "\n",
        "            if cus_additional_info.empty:\n",
        "                len_item = len(data_use_col[item])\n",
        "                if item == 2:\n",
        "                    len_item -= 1\n",
        "                cus_features = np.concatenate((cus_features, [np.nan] * len_item), axis=0)\n",
        "            else:\n",
        "                cur_cus_feature = cus_additional_info.loc[cus_additional_info[date_col[item]].idxmax()]\n",
        "                cur_cus_feature = cur_cus_feature.values[data_use_col[item]]\n",
        "                # 處理 實際金額 = 匯率*金額\n",
        "                if item == 2:\n",
        "                    cur_cus_feature = np.concatenate((cur_cus_feature[:2], [cur_cus_feature[2]*cur_cus_feature[3]], cur_cus_feature[4:]), axis=0)\n",
        "                cus_features = np.concatenate((cus_features, cur_cus_feature), axis=0)\n",
        "\n",
        "        testing_data.append(cus_features)\n",
        "        testing_alert_key.append(alert_key)\n",
        "        # print(cus_features)\n",
        "        print('\\r processing data {}/{}'.format(i+1, df_public_x.shape[0]), end = '')\n",
        "    return np.array(training_data), labels, np.array(testing_data), testing_alert_key"
      ],
      "metadata": {
        "id": "znQLaj3_T5RY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 訓練資料處理"
      ],
      "metadata": {
        "id": "7UCqVIdbdxWz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = './data'\n",
        "# data preprocessing\n",
        "training_data, labels, testing_data, testing_alert_key = preprocess(data_dir)\n",
        "print(training_data[0])\n",
        "print(training_data.shape, testing_data.shape)\n"
      ],
      "metadata": {
        "id": "FZLalEaag9L3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00254942-4b7f-4a0f-8f3b-afd5579deab7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading csv...\n",
            "Start processing training data...\n",
            " processing data 23906/23906Processing time: 5664.036 secs\n",
            "Missing value of 4 csvs: [7214, 9577, 3086, 17719]\n",
            "Start processing testing data\n",
            " processing data 1845/1845[1 17.0 375576.0 4 85428.0 301224.0 154122.0 0.0 0.0 0.0 151434.0 0.0 134\n",
            " 47 673.0 1 2 309.0 1 nan nan 0 1 nan nan]\n",
            "(23906, 25) (1845, 25)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 缺失值補漏\n",
        "  可以發現有不少筆資料其實是有缺漏的，補上缺失值的方法有很多種，我們對於數值類資料補上中位數，對於類別類資料補上眾數。"
      ],
      "metadata": {
        "id": "L_jqrGX0fgsV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "''' Missing Value Imputation '''\n",
        "imp_median = SimpleImputer(missing_values=np.nan, strategy='median')\n",
        "imp_most_frequent = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
        "# for numerical index we do imputation using median\n",
        "numerical_index = [2,4,5,6,7,8,9,10,11,14,17,24]\n",
        "# Otherwise we select the most frequent\n",
        "non_numerical_index = [0,1,3,12,13,15,16,18,19,20,21,22,23]\n",
        "\n",
        "numerical_data = training_data[:, numerical_index]\n",
        "non_numerical_data = training_data[:, non_numerical_index]\n",
        "\n",
        "imp_median.fit(numerical_data)\n",
        "numerical_data = imp_median.transform(numerical_data)\n",
        "\n",
        "imp_most_frequent.fit(non_numerical_data)\n",
        "non_numerical_data = imp_most_frequent.transform(non_numerical_data)\n",
        "\n",
        "training_data = np.concatenate((non_numerical_data, numerical_data), axis=1)"
      ],
      "metadata": {
        "id": "zZ4qVF1oUIss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "  此外，若類別類資料跟數字大小沒關係，我們採用 one-hot encoding 將其編碼。"
      ],
      "metadata": {
        "id": "YM9IdAJF69-t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for some catogorical features, we do one hot encoding\n",
        "one_hot_index = [1,3,4,5,6,7,8,9,12]\n",
        "onehotencorder = ColumnTransformer(\n",
        "    [('one_hot_encoder', OneHotEncoder(handle_unknown='ignore'), one_hot_index)],\n",
        "    remainder='passthrough'                     \n",
        ")\n",
        "onehotencorder.fit(training_data)\n",
        "training_data = onehotencorder.transform(training_data)\n",
        "print(training_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmIpbiEa1WZH",
        "outputId": "7d8fc435-7535-49b5-8162-eb495e05c6f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(23906, 357)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# XGBoost 訓練"
      ],
      "metadata": {
        "id": "g9Eb7vE1fXeZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import xgboost as xgb\n",
        "# 建立 XGBClassifier 模型\n",
        "xgbrModel=xgb.XGBClassifier(random_state=0)\n",
        "# 使用訓練資料訓練模型\n",
        "xgbrModel.fit(training_data, labels)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xg-TcXydbLbj",
        "outputId": "c4c85241-7538-4393-cd4d-c73b0bd8490d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
              "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
              "              early_stopping_rounds=None, enable_categorical=False,\n",
              "              eval_metric=None, feature_types=None, gamma=0, gpu_id=-1,\n",
              "              grow_policy='depthwise', importance_type=None,\n",
              "              interaction_constraints='', learning_rate=0.300000012,\n",
              "              max_bin=256, max_cat_threshold=64, max_cat_to_onehot=4,\n",
              "              max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n",
              "              missing=nan, monotone_constraints='()', n_estimators=100,\n",
              "              n_jobs=0, num_parallel_tree=1, predictor='auto', random_state=0, ...)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 預測與結果輸出\n",
        "  利用訓練好的模型對目標alert key預測報SAR的機率以及輸出為目標格式。\n",
        "  目標輸出筆數3850，其中public筆數為1845筆。\n",
        "  因上傳格式需要private跟public alert key皆考慮，直接從預測範本統計要預測的alert key，預測結果輸出為prediction.csv。"
      ],
      "metadata": {
        "id": "BFmH4_ic8l4e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Do missing value imputation and one-hot encoding for testing data\n",
        "test_numerical_data = testing_data[:, numerical_index]\n",
        "test_non_numerical_data = testing_data[:, non_numerical_index]\n",
        "\n",
        "test_numerical_data = imp_median.transform(test_numerical_data)\n",
        "\n",
        "test_non_numerical_data = imp_most_frequent.transform(test_non_numerical_data)\n",
        "\n",
        "testing_data = np.concatenate((test_non_numerical_data, test_numerical_data), axis=1)\n",
        "testing_data = onehotencorder.transform(testing_data)\n",
        "\n"
      ],
      "metadata": {
        "id": "7HHc2MF1S9EA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read csv of all alert keys need to be predicted\n",
        "public_private_test_csv = os.path.join(data_dir, '預測的案件名單及提交檔案範例.csv')\n",
        "df_public_private_test = pd.read_csv(public_private_test_csv)\n",
        "\n",
        "# Predict probability\n",
        "predicted = []\n",
        "for i, _x in enumerate(xgbrModel.predict_proba(testing_data)):\n",
        "    predicted.append([testing_alert_key[i], _x[1]])\n",
        "predicted = sorted(predicted, reverse=True, key= lambda s: s[1])\n",
        "\n",
        "# 考慮private alert key部分，滿足上傳條件\n",
        "public_private_alert_key = df_public_private_test['alert_key'].values\n",
        "print(len(public_private_alert_key))\n",
        "\n",
        "# For alert key not in public, add zeros\n",
        "for key in public_private_alert_key:\n",
        "    if key not in testing_alert_key:\n",
        "        predicted.append([key, 0])\n",
        "\n",
        "predict_alert_key, predict_probability = [], []\n",
        "for key, prob in predicted:\n",
        "    predict_alert_key.append(key)\n",
        "    predict_probability.append(prob)\n",
        "\n",
        "df_predicted = pd.DataFrame({\n",
        "    \"alert_key\": predict_alert_key,\n",
        "    \"probability\": predict_probability\n",
        "})\n",
        "\n",
        "df_predicted.to_csv('prediction_baseline.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apza1dJrXoe3",
        "outputId": "111f7c7e-4ef9-4ab0-a9f1-4a9597ffd9f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3850\n"
          ]
        }
      ]
    }
  ]
}